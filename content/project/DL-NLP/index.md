---
title: Sequential Models for Text Classification and Generation 
summary: Worked on data pre-processing of Amazon Reviews Dataset and implementing LSTM, BiLSTM, GRU, RNN architectures with Attention modules for review rating prediction using weighted loss and SMOTE techniques to handle class imbalance. Improved the F1 score accuracy by using b-directional transformer-based architectures BERT and RoBERTa. Designed seq2seq architecture with attention trained using teacher forcing strategy to generate summaries of review text.
tags:
- Deep Learning
date: "2020-02-01T00:00:00Z"

# Optional external URL for project (replaces project detail page).
external_link: https://github.com/mihirp98/DL-NLP

image:
  caption: 
  focal_point: Smart

links:
url_pdf: 
url_video: 
---
